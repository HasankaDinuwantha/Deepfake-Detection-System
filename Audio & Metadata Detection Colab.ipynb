{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "executionInfo": {
     "elapsed": 125508,
     "status": "ok",
     "timestamp": 1764642069847,
     "user": {
      "displayName": "Heshan Gunathilaka",
      "userId": "12800214512840931942"
     },
     "user_tz": -330
    },
    "id": "UOE2aKkb2v1Y",
    "outputId": "82aa6fc5-29cd-45a4-e355-d86011bd6d7e"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/duran/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# OPTIMIZED AI VOICE DETECTOR V7 (Sanitized Indentation)\n",
    "# ==============================================================================\n",
    "!pip install -q kaggle librosa soundfile ffmpeg-python gTTS tqdm scikit-learn matplotlib torch torchvision torchaudio\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.amp import autocast, GradScaler\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# CONFIGURATION\n",
    "CONFIG = {\n",
    "    'SR': 16000,\n",
    "    'DURATION': 4.0,\n",
    "    'N_MELS': 80,\n",
    "    'HOP_LEN': 512,\n",
    "    'MAX_SAMPLES': 2000,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 10,\n",
    "    'LR': 1e-3,\n",
    "    'SEED': 42,\n",
    "    'DATASET_SLUG': 'adarshsingh0903/audio-deepfake-detection-dataset'\n",
    "}\n",
    "\n",
    "ROOT_DIR = Path('/content/deepfake_project')\n",
    "RAW_DIR = ROOT_DIR / 'raw'\n",
    "MEL_DIR = ROOT_DIR / 'mels'\n",
    "MODELS_DIR = ROOT_DIR / 'models'\n",
    "\n",
    "# 1. SETUP & CLEANUP\n",
    "print(\"ðŸ§¹ Cleaning up old data...\")\n",
    "if RAW_DIR.exists(): shutil.rmtree(RAW_DIR)\n",
    "if MEL_DIR.exists(): shutil.rmtree(MEL_DIR)\n",
    "for p in [RAW_DIR, MEL_DIR, MODELS_DIR]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Running on: {DEVICE}\")\n",
    "\n",
    "# 2. KAGGLE DOWNLOAD\n",
    "def setup_kaggle():\n",
    "    kaggle_path = Path('/root/.kaggle/kaggle.json')\n",
    "    if kaggle_path.exists(): return True\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"ðŸ“‚ Upload kaggle.json if prompted:\")\n",
    "        uploaded = files.upload()\n",
    "        if 'kaggle.json' in uploaded:\n",
    "            kaggle_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(kaggle_path, 'wb') as f: f.write(uploaded['kaggle.json'])\n",
    "            os.chmod(kaggle_path, 0o600)\n",
    "            return True\n",
    "    except: pass\n",
    "    return False\n",
    "\n",
    "if setup_kaggle():\n",
    "    print(f\"â¬‡ï¸ Downloading Dataset (1GB)...\")\n",
    "    os.system(f\"kaggle datasets download -d {CONFIG['DATASET_SLUG']} -p {RAW_DIR} --unzip\")\n",
    "    print(\"âœ… Download Complete.\")\n",
    "\n",
    "# 3. ROBUST DATA SCANNER\n",
    "def scan_dataset(root):\n",
    "    data = []\n",
    "    print(f\"ðŸ” Scanning {root} for audio files...\")\n",
    "\n",
    "    # Walk through folders\n",
    "    for f in tqdm(root.rglob('*')):\n",
    "        if f.is_file() and f.suffix.lower() in {'.wav', '.mp3', '.flac'}:\n",
    "            path_str = str(f).lower()\n",
    "\n",
    "            label = None\n",
    "            if 'real' in path_str or 'bonafide' in path_str:\n",
    "                label = 0\n",
    "            elif 'fake' in path_str or 'spoof' in path_str:\n",
    "                label = 1\n",
    "\n",
    "            if label is not None:\n",
    "                data.append({'path': str(f), 'label': label})\n",
    "\n",
    "    # Shuffle\n",
    "    random.seed(CONFIG['SEED'])\n",
    "    random.shuffle(data)\n",
    "\n",
    "    df_temp = pd.DataFrame(data)\n",
    "    if df_temp.empty: return df_temp\n",
    "\n",
    "    # Balance and Limit\n",
    "    real_df = df_temp[df_temp['label'] == 0]\n",
    "    fake_df = df_temp[df_temp['label'] == 1]\n",
    "\n",
    "    print(f\"   > Found: Real={len(real_df)}, Fake={len(fake_df)}\")\n",
    "\n",
    "    min_len = min(len(real_df), len(fake_df))\n",
    "    if min_len == 0: return df_temp\n",
    "\n",
    "    limit_per_class = min(min_len, CONFIG['MAX_SAMPLES'] // 2)\n",
    "\n",
    "    df_balanced = pd.concat([\n",
    "        real_df.sample(limit_per_class),\n",
    "        fake_df.sample(limit_per_class)\n",
    "    ])\n",
    "    return df_balanced.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df = scan_dataset(RAW_DIR)\n",
    "print(f\"ðŸ“Š Final Dataset Size: {len(df)}\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "if len(df) < 100:\n",
    "    raise RuntimeError(\"âŒ Dataset download failed or is empty.\")\n",
    "\n",
    "# 4. PREPROCESSING\n",
    "def compute_mels(row):\n",
    "    path, label = row['path'], row['label']\n",
    "    fname = Path(path).stem + \"_\" + str(random.randint(0,9999))\n",
    "    save_path = MEL_DIR / f\"{fname}.npy\"\n",
    "\n",
    "    try:\n",
    "        y, sr = librosa.load(path, sr=CONFIG['SR'], duration=CONFIG['DURATION'])\n",
    "        target_len = int(CONFIG['SR'] * CONFIG['DURATION'])\n",
    "        if len(y) < target_len:\n",
    "            y = np.pad(y, (0, target_len - len(y)))\n",
    "        else:\n",
    "            y = y[:target_len]\n",
    "\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=CONFIG['N_MELS'], hop_length=CONFIG['HOP_LEN'])\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "        np.save(save_path, mel_db.astype(np.float16))\n",
    "        return str(save_path), label\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "print(\"âš™ï¸ Processing Audio (Wait 2-3 mins)...\")\n",
    "processed_data = [compute_mels(row) for _, row in tqdm(df.iterrows(), total=len(df))]\n",
    "df_processed = pd.DataFrame([x for x in processed_data if x[0]], columns=['mel_path', 'label'])\n",
    "\n",
    "# 5. MODEL\n",
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, df): self.df = df\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        mel = np.load(row['mel_path']).astype(np.float32)\n",
    "        norm_mel = (mel - mel.mean()) / (mel.std() + 1e-6)\n",
    "        return torch.tensor(norm_mel).unsqueeze(0), torch.tensor(row['label'])\n",
    "\n",
    "train_size = int(0.8 * len(df_processed))\n",
    "train_ds, val_ds = random_split(VoiceDataset(df_processed), [train_size, len(df_processed)-train_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "class AudioDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = AudioDetector().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['LR'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# 6. TRAINING\n",
    "print(\"ðŸš€ Training Model...\")\n",
    "history = {'loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(CONFIG['EPOCHS']):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            loss = criterion(model(X), y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            all_preds.extend(model(X).argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    history['loss'].append(total_loss/len(train_loader))\n",
    "    history['val_acc'].append(acc)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Val Acc: {acc:.4f}\")\n",
    "\n",
    "# 7. RESULTS\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history['loss'], label='Loss')\n",
    "plt.plot(history['val_acc'], label='Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Training Progress\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "executionInfo": {
     "elapsed": 26625,
     "status": "ok",
     "timestamp": 1764643520785,
     "user": {
      "displayName": "Heshan Gunathilaka",
      "userId": "12800214512840931942"
     },
     "user_tz": -330
    },
    "id": "DQHmV7Zp8saN",
    "outputId": "4567abc1-7bef-4392-d4e6-cad7b5a9e8e3"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ðŸš€ TEST YOUR TRAINED MODEL\n",
    "# ==============================================================================\n",
    "from google.colab import files\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Upload\n",
    "print(\"Upload an audio file (.wav or .mp3):\")\n",
    "uploaded = files.upload()\n",
    "fname = list(uploaded.keys())[0]\n",
    "\n",
    "# 2. Preprocess\n",
    "def prepare_audio(path):\n",
    "    y, sr = librosa.load(path, sr=16000, duration=4.0)\n",
    "    target_len = 16000 * 4\n",
    "    if len(y) < target_len: y = np.pad(y, (0, target_len - len(y)))\n",
    "    else: y = y[:target_len]\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80, hop_length=512)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    norm_mel = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n",
    "    return torch.tensor(norm_mel).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# 3. Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tens = prepare_audio(fname).to(DEVICE)\n",
    "    logits = model(tens)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    pred = torch.argmax(probs).item()\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"ðŸ“ File: {fname}\")\n",
    "print(f\"ðŸ¤– Prediction: {'FAKE (AI)' if pred == 1 else 'REAL (Human)'}\")\n",
    "print(f\"ðŸ“Š Confidence: {probs[0][pred].item()*100:.2f}%\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "executionInfo": {
     "elapsed": 47592,
     "status": "error",
     "timestamp": 1764651270181,
     "user": {
      "displayName": "Heshan Gunathilaka",
      "userId": "12800214512840931942"
     },
     "user_tz": -330
    },
    "id": "b4r5TNAg-HQH",
    "outputId": "01ee9a01-1c11-46e8-f4ea-bcb0c65485a9"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ðŸš€ TEST YOUR TRAINED MODEL\n",
    "# ==============================================================================\n",
    "from google.colab import files\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Upload\n",
    "print(\"Upload an audio file (.wav or .mp3):\")\n",
    "uploaded = files.upload()\n",
    "fname = list(uploaded.keys())[0]\n",
    "\n",
    "# 2. Preprocess\n",
    "def prepare_audio(path):\n",
    "    y, sr = librosa.load(path, sr=16000, duration=4.0)\n",
    "    target_len = 16000 * 4\n",
    "    if len(y) < target_len: y = np.pad(y, (0, target_len - len(y)))\n",
    "    else: y = y[:target_len]\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80, hop_length=512)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    norm_mel = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n",
    "    return torch.tensor(norm_mel).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# 3. Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tens = prepare_audio(fname).to(DEVICE)\n",
    "    logits = model(tens)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    pred = torch.argmax(probs).item()\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"ðŸ“ File: {fname}\")\n",
    "print(f\"ðŸ¤– Prediction: {'FAKE (AI)' if pred == 1 else 'REAL (Human)'}\")\n",
    "print(f\"ðŸ“Š Confidence: {probs[0][pred].item()*100:.2f}%\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1764644064901,
     "user": {
      "displayName": "Heshan Gunathilaka",
      "userId": "12800214512840931942"
     },
     "user_tz": -330
    },
    "id": "BUox-njF-3DL",
    "outputId": "42d439ee-7a59-405b-acb3-32a5dd053453"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ðŸ’¾ EXPORT AS PYTORCH (.pth)\n",
    "# ==============================================================================\n",
    "from google.colab import files\n",
    "\n",
    "# 1. Save the model weights\n",
    "save_path = \"audio_deepfake_detector.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Model saved to {save_path}\")\n",
    "\n",
    "# 2. Download to your local computer\n",
    "files.download(save_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM/bWbnGJADUZKMEXuZ16TP",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
